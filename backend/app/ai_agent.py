"""
Servicio de Diagn√≥stico offline
Soporta texto, im√°genes y an√°lisis de datos del motor
CON CAPACIDAD DE CONTROLAR EL SISTEMA (umbrales, MQTT, mantenimiento)
"""
import base64
import json
from typing import Optional, List, Dict, Any
import httpx
from datetime import datetime
import re

class DiagnosticService:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model = "diagnostic_model"  # Modelo por defecto
        self.vision_model = "vision_model"  # Modelo para im√°genes
        
        # Herramientas disponibles para el servicio
        self.tools = {
            "modificar_umbrales": {
                "description": "Modifica los umbrales de temperatura, vibraci√≥n o RPM",
                "parameters": ["tipo", "warning", "critical"],
                "example": "modificar_umbrales(tipo='temperatura', warning=70, critical=90)"
            },
            "enviar_comando_mqtt": {
                "description": "Env√≠a comandos MQTT al motor (start, stop, sampling_rate)",
                "parameters": ["topic", "valor"],
                "example": "enviar_comando_mqtt(topic='motor/control/start', valor='1')"
            },
            "obtener_umbrales": {
                "description": "Obtiene los umbrales actuales del sistema",
                "parameters": [],
                "example": "obtener_umbrales()"
            }
        }
        
    async def check_connection(self) -> bool:
        """Verifica si el servicio est√° corriendo"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except:
            return False
    
    async def list_models(self) -> List[str]:
        """Lista modelos disponibles"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    return [model["name"] for model in data.get("models", [])]
        except:
            return []
        return []
    
    def build_motor_context(self, motor_data: Optional[Dict] = None) -> str:
        """Construye contexto del motor para el prompt"""
        if not motor_data:
            return ""
        
        context = "\n\n### Datos actuales del motor:\n"
        
        if "phaseData" in motor_data:
            context += "\n**Datos por fase:**\n"
            for phase, data in motor_data["phaseData"].items():
                context += f"- Fase {phase}:\n"
                context += f"  - Voltaje: {data.get('voltaje', 0)}V\n"
                context += f"  - Corriente: {data.get('corriente', 0)}A\n"
                context += f"  - Potencia: {data.get('potencia', 0)}W\n"
                context += f"  - Factor de Potencia: {data.get('factorPotencia', 0)}\n"
        
        if "generalData" in motor_data:
            gen = motor_data["generalData"]
            context += f"\n**Datos generales:**\n"
            context += f"- Temperatura: {gen.get('temperatura', 0)}¬∞C\n"
            context += f"- RPM: {gen.get('rpm', 0)}\n"
            context += f"- Vibraci√≥n: {gen.get('vibracion', 0)} mm/s\n"
        
        return context
    
    async def chat(
        self,
        message: str,
        image_base64: Optional[str] = None,
        motor_data: Optional[Dict] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> Dict:
        """
        Env√≠a mensaje al servicio de diagn√≥stico
        
        Args:
            message: Mensaje del usuario
            image_base64: Imagen en base64 (opcional)
            motor_data: Datos del motor para contexto (opcional)
            conversation_history: Historial de conversaci√≥n (opcional)
        
        Returns:
            Dict con respuesta y metadata
        """
        try:
            # Seleccionar modelo seg√∫n si hay imagen
            model = self.vision_model if image_base64 else self.model
            
            # Construir sistema prompt especializado en motores
            system_prompt = """Eres un asistente t√©cnico experto en motores el√©ctricos trif√°sicos. Tu estilo es profesional pero cercano y amigable.

**üî• MUY IMPORTANTE - PUEDES CONTROLAR EL SISTEMA:**
Cuando el usuario pida modificar umbrales, temperatura, vibraci√≥n o RPM, DEBES incluir en tu respuesta el c√≥digo de acci√≥n.

**T√âCNICAS DE MANTENIMIENTO DISPONIBLES:**
1. **Arranque Supervisado** (startup_analysis): Monitoreo intensivo durante encendido. Aumenta muestreo a 100ms.
2. **An√°lisis de Carga** (load_analysis): Enciende motor y monitorea estabilizaci√≥n de corriente/potencia.
3. **Test de Vibraci√≥n** (vibration_test): Monitorea vibraci√≥n durante arranque y operaci√≥n.
4. **Inspecci√≥n Completa** (full_inspection): Ciclo completo de diagn√≥stico.

**FORMATO OBLIGATORIO para ejecutar acciones:**

Para modificar umbrales:
ACCION{action:modificar_umbrales,tipo:temperatura,warning:70,critical:90}

Para ejecutar t√©cnica de mantenimiento:
ACCION{action:ejecutar_tecnica,tecnica:startup_analysis}

Para control de motor:
ACCION{action:mqtt,topic:motor/control/start,value:1}
ACCION{action:mqtt,topic:motor/control/stop,value:0}
ACCION{action:mqtt,topic:motor/control/sampling_rate,value:500}

**Ejemplos REALES:**

Usuario: "quiero iniciar una t√©cnica de mantenimiento"
Tu respuesta:
üîß ¬øQu√© t√©cnica deseas ejecutar?
- **Arranque Supervisado**: Para analizar el encendido
- **An√°lisis de Carga**: Para verificar consumo
- **Test de Vibraci√≥n**: Para detectar desbalanceos
- **Inspecci√≥n Completa**: Diagn√≥stico completo

Usuario: "ejecuta arranque supervisado"
Tu respuesta:
üöÄ Iniciando Arranque Supervisado...
ACCION{action:ejecutar_tecnica,tecnica:startup_analysis}
‚úÖ T√©cnica iniciada. Monitoreando encendido del motor.

Usuario: "ajusta la temperatura a 85 grados"
Tu respuesta:
üîß Ajustando umbrales de temperatura a 85¬∞C...
ACCION{action:modificar_umbrales,tipo:temperatura,warning:75,critical:85}
‚úÖ Listo! Temperatura cr√≠tica configurada en 85¬∞C

Usuario: "enciende el motor"
Tu respuesta:
‚ñ∂Ô∏è Encendiendo motor...
ACCION{action:mqtt,topic:motor/control/start,value:1}
Motor encendido

**TIPOS v√°lidos:**
- tipo:temperatura (para temp_warning y temp_critical)
- tipo:vibracion (para vibration_warning y vibration_critical)
- tipo:rpm (para rpm_warning y rpm_critical)
- tecnica: startup_analysis, load_analysis, vibration_test, full_inspection

**Reglas:**
1. **SIEMPRE** incluye la l√≠nea ACCION{} cuando modifiques algo o ejecutes t√©cnicas
2. El warning SIEMPRE es menor que el critical
3. Respuestas de 3-4 l√≠neas m√°ximo
4. Usa emojis: üîß‚úÖ‚ùå‚ö†Ô∏èüî•üìäüõë‚ñ∂Ô∏èüöÄ
5. Responde en espa√±ol

Si el usuario pregunta algo normal (sin pedir cambios), solo responde sin ACCION{}."""  

            # Agregar contexto del motor si est√° disponible
            full_message = message
            if motor_data:
                motor_context = self.build_motor_context(motor_data)
                full_message = f"{message}{motor_context}"
            
            # Construir prompt completo con system + historial + mensaje
            # Usar /api/generate en lugar de /api/chat para mejor control
            conversation_text = system_prompt + "\n\n"
            
            if conversation_history:
                for msg in conversation_history[-5:]:  # √öltimos 5 mensajes
                    role = "Usuario" if msg.get("role") == "user" else "Asistente"
                    conversation_text += f"{role}: {msg.get('content', '')}\n\n"
            
            conversation_text += f"Usuario: {full_message}\n\nAsistente:"
            
            # Preparar payload
            payload = {
                "model": model,
                "prompt": conversation_text,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_ctx": 4096
                }
            }
            
            # Si hay imagen, usar endpoint de chat con im√°genes
            if image_base64:
                # Para im√°genes, usar /api/chat con formato especial
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": full_message, "images": [image_base64]}
                    ],
                    "stream": False
                }
                endpoint = "/api/chat"
            else:
                endpoint = "/api/generate"
            
            # Realizar request a Ollama
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.base_url}{endpoint}",
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    # Manejar respuesta seg√∫n el endpoint usado
                    if endpoint == "/api/chat":
                        response_text = result["message"]["content"]
                    else:
                        response_text = result["response"]
                    
                    return {
                        "success": True,
                        "response": response_text,
                        "model": model,
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    return {
                        "success": False,
                        "error": f"Error {response.status_code}: {response.text}",
                        "timestamp": datetime.now().isoformat()
                    }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def analyze_csv_data(self, csv_content: str, question: str) -> Dict:
        """Analiza datos CSV y responde pregunta espec√≠fica"""
        prompt = f"""Analiza los siguientes datos CSV del motor el√©ctrico:

```csv
{csv_content[:2000]}  # Limitar a primeras l√≠neas
```

Pregunta: {question}

Proporciona un an√°lisis detallado identificando:
1. Tendencias
2. Valores anormales
3. Posibles problemas
4. Recomendaciones"""

        return await self.chat(prompt)
    
    async def generate_diagnosis(self, motor_data: Dict) -> Dict:
        """Genera diagn√≥stico autom√°tico basado en datos actuales"""
        prompt = """Analiza el estado actual del motor y proporciona:

1. **Estado general**: ¬øEl motor est√° operando normalmente?
2. **Par√°metros cr√≠ticos**: ¬øHay valores fuera de rango?
3. **Nivel de riesgo**: Bajo/Medio/Alto/Cr√≠tico
4. **Recomendaciones**: Acciones inmediatas o a corto plazo
5. **Predicci√≥n**: ¬øSe prev√© alguna falla?"""

        return await self.chat(prompt, motor_data=motor_data)
    
    def parse_action_from_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Detecta si la IA quiere ejecutar una acci√≥n del sistema
        Busca patrones como: ACCION{tipo:valor}
        """
        # Patr√≥n para detectar acciones
        action_pattern = r'ACCION\{([^}]+)\}'
        match = re.search(action_pattern, response)
        
        if match:
            try:
                action_str = match.group(1)
                # Parsear la acci√≥n (formato: tipo:valor,param:valor)
                parts = action_str.split(',')
                action_data = {}
                for part in parts:
                    key, val = part.split(':', 1)
                    action_data[key.strip()] = val.strip()
                
                return action_data
            except:
                return None
        
        return None
    
    async def execute_system_action(self, action: Dict[str, Any]) -> Dict:
        """Ejecuta una acci√≥n en el sistema (modificar umbrales, MQTT, etc)"""
        action_type = action.get('action')
        
        if action_type == 'modificar_umbrales':
            # Llamar al endpoint de umbrales
            return {
                "success": True,
                "action": "modificar_umbrales",
                "message": f"Umbrales modificados: {action}",
                "data": action
            }
        
        elif action_type == 'mqtt':
            # Enviar comando MQTT
            return {
                "success": True,
                "action": "mqtt",
                "message": f"Comando MQTT enviado: {action.get('topic')} = {action.get('value')}",
                "data": action
            }
        
        else:
            return {
                "success": False,
                "error": f"Acci√≥n desconocida: {action_type}"
            }
    
    async def generate_report(
        self,
        prompt: str,
        statistics: Dict,
        time_range: Dict,
        readings_count: int,
        alerts_count: int,
        requested_variables: list = None
    ) -> str:
        """
        Genera un reporte t√©cnico basado en estad√≠sticas del motor
        
        Args:
            prompt: Solicitud del usuario
            statistics: Estad√≠sticas calculadas del per√≠odo
            time_range: Rango de tiempo analizado
            readings_count: N√∫mero de lecturas
            alerts_count: N√∫mero de alertas
            requested_variables: Lista de variables espec√≠ficas solicitadas
        
        Returns:
            Texto del reporte generado
        """
        
        # Detectar si el usuario solicita an√°lisis temporal
        temporal_keywords = ['hora', 'horas', 'cr√≠tica', 'cr√≠tico', 'momento', 'cuando', 'per√≠odo', 'tiempo']
        needs_temporal = any(keyword in prompt.lower() for keyword in temporal_keywords)
        
        # Variables solicitadas por el usuario
        requested_variables = requested_variables or ['all']
        is_focused = 'all' not in requested_variables
        
        # Construir mensaje de enfoque si solo pidi√≥ variables espec√≠ficas
        focus_message = ""
        if is_focused:
            vars_text = ", ".join(requested_variables)
            focus_message = f"""
**‚ö†Ô∏è IMPORTANTE - ENFOQUE ESPEC√çFICO:**
El usuario SOLO pidi√≥ informaci√≥n sobre: {vars_text.upper()}
NO incluyas informaci√≥n sobre otras variables que no fueron solicitadas.
Enf√≥cate √öNICAMENTE en las variables mencionadas."""
        
        system_prompt = f"""Eres un ingeniero el√©ctrico experto en mantenimiento de motores trif√°sicos. 
Genera reportes t√©cnicos profesionales pero f√°ciles de entender.
{focus_message}

**FORMATO DEL REPORTE:**

# üìä REPORTE DE MONITOREO DE MOTOR{' - ' + vars_text.upper() if is_focused else ''}

## Per√≠odo Analizado
[fechas y duraci√≥n]

## Resumen Ejecutivo
[3-4 l√≠neas resumen general{' enfocado en ' + vars_text if is_focused else ''}]

## An√°lisis Detallado

{'### üå°Ô∏è Temperatura' if not is_focused or 'temperatura' in requested_variables else ''}
{'''- Promedio: X¬∞C
- Rango: X¬∞C - X¬∞C
- Estado: [Normal / Atenci√≥n / Cr√≠tico]
- Recomendaciones: [si aplica]''' if not is_focused or 'temperatura' in requested_variables else ''}

{'### ‚ö° Vibraci√≥n' if not is_focused or 'vibracion' in requested_variables else ''}
{'''- Promedio: X mm/s
- Rango: X - X mm/s
- Estado: [evaluaci√≥n]
- Observaciones: [si aplica]''' if not is_focused or 'vibracion' in requested_variables else ''}

{'### üîÑ Velocidad (RPM)' if not is_focused or 'rpm' in requested_variables else ''}
{'''- Promedio: X RPM
- Estabilidad: [evaluaci√≥n]''' if not is_focused or 'rpm' in requested_variables else ''}

{'### ‚ö° Fases El√©ctricas' if not is_focused or any(v in requested_variables for v in ['fases', 'voltaje', 'corriente']) else ''}
{'''- Fase A: Voltaje X V, Corriente X A
- Fase B: Voltaje X V, Corriente X A
- Fase C: Voltaje X V, Corriente X A
- Desbalance: [an√°lisis]''' if not is_focused or any(v in requested_variables for v in ['fases', 'voltaje', 'corriente']) else ''}

{'''### ‚è∞ HORAS CR√çTICAS DETECTADAS
**IMPORTANTE:** Si el usuario pidi√≥ informaci√≥n sobre horas cr√≠ticas o momentos espec√≠ficos,
DEBES incluir esta secci√≥n listando cada hora problem√°tica con formato:

- **[HORA espec√≠fica]**: Descripci√≥n clara del problema
  - Vibraci√≥n: X mm/s (m√°ximo detectado)
  - Temperatura: Y¬∞C (m√°ximo detectado)
  - Alertas: Z (cr√≠ticas: N)
  - Evaluaci√≥n: [Descripci√≥n del riesgo]

Los datos incluyen "critical_hours" con esta informaci√≥n.''' if needs_temporal else ''}

### üö® Alertas
- Total: X alertas
- Cr√≠ticas: X
- Advertencias: X
- Categor√≠as principales: [lista]

## Conclusiones
[2-3 puntos clave]

## Recomendaciones
[acciones sugeridas]

---
Reporte generado el: [fecha]

**Usa emojis, s√© conciso y t√©cnico. Destaca problemas potenciales.
{'**CR√çTICO: El usuario pidi√≥ informaci√≥n sobre HORAS CR√çTICAS - debes incluir la secci√≥n con horarios espec√≠ficos.**' if needs_temporal else ''}
{'**‚ö†Ô∏è SOLO habla de: ' + vars_text.upper() + ' - NO menciones otras variables.**' if is_focused else ''}**"""

        # Construir el prompt con datos
        temporal_reminder = ""
        if needs_temporal and statistics.get('critical_hours'):
            temporal_reminder = f"""

‚ö†Ô∏è **ATENCI√ìN - AN√ÅLISIS TEMPORAL SOLICITADO:**
El usuario pidi√≥ espec√≠ficamente informaci√≥n sobre horas cr√≠ticas. Los datos incluyen:
- {len(statistics.get('critical_hours', []))} horas cr√≠ticas identificadas
- An√°lisis hora por hora en "hourly_analysis"
- Detalles de cada hora cr√≠tica en "critical_hours"

DEBES crear la secci√≥n "HORAS CR√çTICAS DETECTADAS" con cada hora listada individualmente."""
        
        # Mensaje adicional si solo pidi√≥ variables espec√≠ficas
        focus_reminder = ""
        if is_focused:
            focus_reminder = f"""

üéØ **RECORDATORIO DE ENFOQUE:**
El usuario pidi√≥ SOLAMENTE: {vars_text.upper()}
Los datos estad√≠sticos solo incluyen esas variables.
NO inventes ni menciones datos sobre otras variables.
Limita tu an√°lisis exclusivamente a lo solicitado."""
        
        data_context = f"""
**Solicitud del usuario:** {prompt}

**Rango de tiempo:** Del {time_range['start']} al {time_range['end']}

**Datos recopilados:**
- Total de lecturas: {readings_count}
- Total de alertas: {alerts_count}

**Estad√≠sticas:**
```json
{json.dumps(statistics, indent=2)}
```
{temporal_reminder}
{focus_reminder}

Genera un reporte profesional basado en estos datos."""

        full_prompt = f"{system_prompt}\n\n{data_context}\n\nReporte:"
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,  # M√°s determin√≠stico para reportes
                            "top_p": 0.9,
                            "num_ctx": 8192  # M√°s contexto para reportes largos
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "Error: No se pudo generar el reporte")
                else:
                    return f"Error al generar reporte: {response.status_code}"
        
        except Exception as e:
            return f"Error al conectar con Ollama: {str(e)}"
